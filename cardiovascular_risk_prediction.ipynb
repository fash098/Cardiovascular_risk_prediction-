{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "w6K7xa23Elo4",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "Yfr_Vlr8HBkt",
        "578E2V7j08f6",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fash098/Cardiovascular_risk_prediction-/blob/main/cardiovascular_risk_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Cardiovascular Risk Prediction\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification\n",
        "##### **Contribution**    - Individual\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aimed to predict the 10-year risk of coronary heart disease (CHD) using Machine Learning and data from a cardiovascular study in Framingham, Massachusetts. The dataset included information on 4,000+ patients and 15 attributes representing potential risk factors for CHD.\n",
        "\n",
        "Data preprocessing involved cleaning, transforming, and handling missing values and outliers. Skewed variables were transformed for better model performance. Feature selection removed multicollinearity and created a new feature called pulse pressure. The most important features for CHD risk prediction were identified.\n",
        "\n",
        "To address the imbalanced dataset, the SMOTE combined with Tomek links undersampling technique was used. Standard scaling ensured all features were on the same scale. Several machine learning models were evaluated based on recall, with the Neural Network (tuned) selected as the final model due to its high recall score. This choice aimed to identify as many patients at risk of CHD as possible, even with some false positives.\n",
        "\n",
        "Overall, the project demonstrated the potential of machine learning to accurately predict CHD risk using data from a cardiovascular study. By preprocessing data, selecting relevant features, and choosing an appropriate model based on evaluation metrics, accurate CHD risk prediction was achieved, leading to positive impact in healthcare."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Cardiovascular diseases (CVDs) are a group of disorders of the heart and blood vessels. They include:\n",
        "\n",
        "**Coronary Heart Disease** – a disease of the blood vessels supplying the heart muscle;\n",
        "\n",
        "** Cerebrovascular disease ** – a disease of the blood vessels supplying the brain;\n",
        "\n",
        "** Peripheral arterial disease **  – a disease of blood vessels supplying the arms and legs;\n",
        "\n",
        "** Rheumatic heart disease **  – damage to the heart muscle and heart valves from rheumatic fever, caused by streptococcal bacteria;\n",
        "\n",
        "** Congenital heart disease**  – birth defects that affect the normal development and functioning of the heart caused by malformations of the heart structure from birth; and\n",
        "\n",
        "**Deep vein thrombosis and pulmonary embolism** – blood clots in the leg veins, which can dislodge and move to the heart and lungs.\n",
        "\n",
        "\n",
        "This dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has a 10-year risk of future coronary heart disease (CHD). The dataset provides the patients' information. It includes over 4,000 records and 15 attributes. Each attribute is a potential risk factor including but not limited to demographic, behavioral, and medical risk factors."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime as dt\n",
        "\n",
        "\n",
        "# Import Visualization Libraries\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Import Machine Learning Libraries\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "import xgboost as xgb\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "\n",
        "# Import warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading dataset stored in google drive(csv file format) to collab after mounting the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5-pLYRudDKRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_file =('/content/drive/MyDrive/AlmaBetter/Capstone_Project/data_cardiovascular_risk.csv')\n",
        "cvd_df = pd.read_csv(path_file)"
      ],
      "metadata": {
        "id": "2XRX5-O6Durd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "cvd_df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cvd_df.tail()"
      ],
      "metadata": {
        "id": "AKC-JpmFE4tK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "cvd_df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "cvd_df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(cvd_df[cvd_df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(cvd_df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(cvd_df.isnull(), cbar=False)\n"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The dataset will be helpful in predicting cardiovascular disease risk prediction of individuals in next 10 years.\n",
        "\n",
        "- It contains 3390 rows, 17 columns (16 features and 1 label). Data contains a mix of int, float and object datatypes.\n",
        "\n",
        "- There are 0 duplicate values in the data. However, it contains missing values inder multiple features such as education, cigsPerDay, BPMeds, totChol, BMI, heartRate and glucose. This needs to be handled efficiently."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "cvd_df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "cvd_df.describe(include ='all')"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "for i in cvd_df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",cvd_df[i].nunique())"
      ],
      "metadata": {
        "id": "Ek9hN8GBarCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Unique values under education.\n",
        "np.unique(cvd_df['education'])"
      ],
      "metadata": {
        "id": "SKhKrR2Fa23F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description\n",
        "\n"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables can be divided into 5 categories such as :\n",
        "\n",
        "**Demographic:**\n",
        "\n",
        "1) Sex: Male or Female (\"M\" or \"F\")\n",
        "\n",
        "2) Age: Patient's age (Continuous)\n",
        "\n",
        "3) Education: Patient's level of education (categorical values - 1,2,3,4)\n",
        "\n",
        "**Behavioral:**\n",
        "\n",
        "4) is_smoking: if the patient is a current smoker or not (\"YES\" or \"NO\")\n",
        "\n",
        "5) Cigs Per Day: the number of cigarettes the patient smokes per day.\n",
        "(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n",
        "\n",
        "**Medical (history):**\n",
        "\n",
        "6) BP Meds: If a patient is on Blood pressure regulating medication or not(Nominal)\n",
        "\n",
        "7) Prevalent Stroke: if the patient had previously had a stroke or not(Nominal)\n",
        "\n",
        "8) Prevalent Hyp: if the patient was hypertensive or not (Nominal)\n",
        "\n",
        "9) Diabetes: if the patient had diabetes or not (Nominal)\n",
        "\n",
        "**Medical (current):**\n",
        "\n",
        "10) Tot Chol: total cholesterol level (Continuous)\n",
        "\n",
        "11) Sys BP: systolic blood pressure (Continuous)\n",
        "\n",
        "12) Dia BP: diastolic blood pressure (Continuous)\n",
        "\n",
        "13) BMI: Body Mass Index (Continuous)\n",
        "\n",
        "14) Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of large number of possible values.)\n",
        "\n",
        "15) Glucose: glucose level (Continuous)\n",
        "\n",
        "**Predict variable (target):**\n",
        "\n",
        "16) 10-year risk of coronary heart disease CHD(binary: “1”, means “Yes”, “0” means “No”)"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Before performing any sort of data wrangling/ data manipulations, it is always advisable to make a copy of your data.\n",
        "df = cvd_df.copy()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "mKhNsKBGYgz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming some of the variables to make it more readable\n",
        "\n",
        "df.rename(columns={'cigsPerDay':'cigs_per_day','BPMeds':'bp_meds',\n",
        "                   'prevalentStroke':'prevalent_stroke','prevalentHyp':'prevalent_hyp',\n",
        "                   'totChol':'total_cholesterol','sysBP':'systolic_bp','diaBP':'diastolic_bp',\n",
        "                   'BMI':'bmi','heartRate':'heart_rate','TenYearCHD':'ten_year_chd'},\n",
        "          inplace = True)\n"
      ],
      "metadata": {
        "id": "k1d0y5_cZICA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping the id column as it is not relevant\n",
        "\n",
        "df.drop(columns=['id'], inplace=True)"
      ],
      "metadata": {
        "id": "KH5xq2oROQae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "qJTyZhYE6azB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We have made a copy of the dataset and preceeded to rename of the features and the label to make the variables more readable. We have also gathered some information about each of the variables.\n",
        "\n",
        "- Dropped id column as we have the index in same order\n",
        "\n",
        "- We are yet to handle missing values which will be taken care of as we proceed."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 | Pie Chart"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Understanding the dependent variable (Target variable ; ten_year_chd)\n",
        "\n",
        "colors = sns.color_palette(\"pastel\")\n",
        "plt.figure(figsize=(9,7))\n",
        "explode = [0,0.1]\n",
        "textprops = {'fontsize':10}\n",
        "plt.pie(df['ten_year_chd'].value_counts(), labels=['Not CHD(%)','CHD(%)'], startangle=90, colors=colors, explode = explode, autopct=\"%1.1f%%\",textprops = textprops)\n",
        "plt.title('Ten Year CHD (%)', fontsize=20)\n",
        "\n",
        "# displaying chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?\n",
        "A data's part-to-whole relationship is illustrated with a pie chart. The area covered in a circle with various colours makes it simple to illustrate how the percentages compare. When comparing different percentages, pie charts are widely utilised. I thus developed a pie chart, which enabled me to compare the percentage of patients susceptible to CVD in next 10 years to patients who are not."
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the data it is anticipated that 15.1% of patients among the 3390 are susceptible cardiovascular diseases in the coming 10 years. Whereas 84.9 % of the patients are not."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The gained insight necessarily doesn't imply any negative growth in terms of business perspective."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 | Bar Plot"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "\n",
        "# Binning the features into different categories depending upon their unique value to undersand the distributions.\n",
        "\n",
        "categorical_var = [i for i in df.columns if df[i].nunique()<=4]\n",
        "continuous_var = [i for i in df.columns if i not in categorical_var]\n",
        "dependent_var = ['ten_year_chd']\n",
        "\n",
        "print(categorical_var )\n",
        "print(continuous_var)\n",
        "print(dependent_var)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of categorical_var\n",
        "# Analysing the distribution of categorical variables in the dataset\n",
        "\n",
        "for i in categorical_var:\n",
        "  plt.figure(figsize=(10,5))\n",
        "  p = sns.countplot(x=i, data = df)\n",
        "  plt.xlabel(i)\n",
        "  plt.title(i+' distribution')\n",
        "  for i in p.patches:\n",
        "    p.annotate(f'{i.get_height()}', (i.get_x() + i.get_width() / 2., i.get_height()), ha = 'center', va = 'center', xytext = (0, 8), textcoords = 'offset points')\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "oWjgPlxgrryb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar charts are used to compare the size or frequency of different categories or groups of data. Bar charts are useful for comparing data across different categories, and they can be used to display a large amount of data in a small space.\n"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The distribution of different categories in the categorical columns can be seen. The education column has the highest for the 1 category followed by 2 3 and 4.\n",
        "\n",
        "- The gender distribution is not even. Higher count can be seen for females.\n",
        "\n",
        "- is_smoking column looks almost even.\n",
        "\n",
        "- Bp_meds, prevalent_stroke, prevalent_hyp and diabetes are imbalanced, they have comparitively fewer counts for the positive cases.\n",
        "\n",
        "- ten_year_chd is also imbalanced with fewer positive cases compared to the negative cases."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Understanding the distribution of categories in various columns helps identify patterns and target specific demographics or areas of focus.\n",
        "  - For example, businesses can develop tailored marketing campaigns based on the gender distribution or design educational programs based on the education levels of the target audience.\n",
        "- Additionally, recognizing the imbalanced distribution of health conditions can guide businesses in developing specialized treatments or preventive measures to address specific needs.\n"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3 | Distribution plot"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "plt.figure(figsize=(18,12))\n",
        "for n,column in enumerate(continuous_var):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.distplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dist plot or distribution plot assist us in identifying outliers and skewness or providing a summary of the measures of central tendency (mean, median, and mode). This is extremely helpful in understanding the general distribution of each feature in the dataset."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are able to detect skewness in the data and presence of outliers is also seen."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No significant business impact"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 100% stacked bar chart\n",
        "\n",
        "for i in categorical_var[:-1]:\n",
        "    x_var, y_var = i, dependent_var[0]\n",
        "    plt.figure(figsize=(10,5))\n",
        "    df_grouped = df.groupby(x_var)[y_var].value_counts(normalize=True).unstack(y_var)*100\n",
        "    df_grouped.plot.barh(stacked=True)\n",
        "    plt.legend(\n",
        "        bbox_to_anchor=(1.05, 1),\n",
        "        loc=\"upper left\",\n",
        "        title=y_var)\n",
        "\n",
        "    plt.title(\"% of patients at the risk of CHD by: \"+i)\n",
        "    for ix, row in df_grouped.reset_index(drop=True).iterrows():\n",
        "        # print(ix, row)\n",
        "        cumulative = 0\n",
        "        for element in row:\n",
        "            if element > 0.1:\n",
        "                plt.text(\n",
        "                    cumulative + element / 2,\n",
        "                    ix,\n",
        "                    f\"{int(element)} %\",\n",
        "                    va=\"center\",\n",
        "                    ha=\"center\",\n",
        "                )\n",
        "            cumulative += element\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "AgN8BNpTTKwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 100% horizontal stacked bar chart is a type of data visualization that represents the composition or proportion of multiple categories within a whole. Can be used for comparison of proportions, percentage representation, distribution across categories, ect..\n"
      ],
      "metadata": {
        "id": "KyLzPlhTDy6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The percentage of CHD as per education does not provide much information as it is similar throughout the different education categories.\n",
        "\n",
        "- It can be observed under sex we that there is a slightly higher chance of CHD in male than in female.\n",
        "\n",
        "- In is_smoking again slightly higher percentage is seen for observations under is_smoking.\n",
        "\n",
        "- Under bp_meds and diabetes, we see higher percentage of CHD for positive cases compared to the negative case.\n",
        "\n",
        "- Finally for the positive prevalent_stroke, the percentage is almost half indicating that the positive CHD is high for positive prevalent_stroke."
      ],
      "metadata": {
        "id": "u_cOR9TND0xp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 | Box Plot"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18,12))\n",
        "for n,column in enumerate(continuous_var):\n",
        "  plt.subplot(5, 4, n+1)\n",
        "  sns.boxplot(df[column])\n",
        "  plt.title(f'{column.title()}',weight='bold')\n",
        "  plt.tight_layout()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Box Plot is the visual representation of the statistical five number summary of a given data set.\n",
        "\n",
        "A Five Number Summary includes:\n",
        "\n",
        "- Minimum\n",
        "- First Quartile\n",
        "- Median (Second Quartile)\n",
        "- Third Quartile\n",
        "- Maximum"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 | Correlation Heat Map\n"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Defining a Seaborn correlation map(Heatmap).\n",
        "correlmap = df.corr()\n",
        "\n",
        "# display the heatmap.\n",
        "f, ax = plt.subplots(figsize=(14, 8))\n",
        "sns.heatmap(correlmap, annot=True, ax = ax)"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation heatmaps are a type of plot that visualize the strength of relationships between numerical variables. Correlation plots are used to understand which variables are related to each other and the strength of this relationship. We used correlation heatmap here to check if some variables have any direct correlation with other variables. Correlation value ranges between -1 and 1. Positive value shows postive correlation and negative value shows negative correlation."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- From the correlation graph it is understood that glucose and diabetes is highly correlated.\n",
        "\n",
        "- Similarly important correlation is observed between systolic_bp, diastolic_bp and prevalent_hypertension.\n",
        "\n",
        "We can treat this during feature engineering."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No specific insights for negative growth"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "\n",
        "# pairplot with hue sex\n",
        "sns.pairplot(df, hue ='ten_year_chd')\n",
        "# to show\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A pairplot, also known as a scatterplot matrix, is a visualization that allows you to visualize the relationships between all pairs of variables in a dataset. It is a useful tool for data exploration because it allows you to quickly see how all of the variables in a dataset are related to one another.\n",
        "\n",
        "Thus, we used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the cigs_per_day data distribution is highly skewed and it contains high 0 value so we can convert this into categorical column."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Null hypothesis: There is no association between education level and Coronary Heart Disease outcome.\n",
        "\n",
        "Alternate hypothesis: There is an association between education level and Coronary Heart disease outcome.\n",
        "\n"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "import pandas as pd\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = pd.crosstab(df['education'], df['ten_year_chd'])\n",
        "print(contingency_table)\n",
        "\n",
        "# Perform chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Print p-value\n",
        "print(f'p-value: {p}')\n",
        "\n",
        "#The p value is significantly lower than 0.05 so we reject the null hypothesis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To test the hypothesis that the ‘education’ column does not impact the outcome of chronic heart disease (CHD), I performed a chi-squared test of independence. This statistical test allowed me to determine if there was a significant association between education level and CHD outcome. By calculating the chi-squared statistic and p-value, I was able to make a statistical inference about the relationship between these two variables in our dataset."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used the chi-squared test of independence to check if the 'education' column has an impact on chronic heart disease (CHD) outcome. This test is suitable when we want to see if there is a significant connection between two categories. In our case, both education level and CHD outcome are categories, so the chi-squared test is a good choice.\n",
        "\n",
        "The chi-squared test compares the actual distribution of data in a table with the expected distribution assuming the null hypothesis is true. If there is a notable difference between the actual and expected distributions, it suggests that there is a link between the two variables.\n",
        "\n",
        "In summary, I chose the chi-squared test of independence because it is a commonly used statistical test for studying the relationship between two categories. It allowed me to make a statistical conclusion about how education level and CHD outcome relate in our dataset.\n"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "print(df.isnull().sum()/len(df)*100)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** Missing value prevalance in each category with it's percentage :**\n",
        "\n",
        "- glucose  - 304 - 8.9% (continuous)\n",
        "\n",
        "- education - 87 - 2.5% (categorical)\n",
        "\n",
        "- bp_meds - 44 - 1.2% (categorical)\n",
        "\n",
        "- total_cholestrol - 38 - 1.1% (continuous)\n",
        "\n",
        "- cigs_per_day - 22 - 0.6% (continuous)\n",
        "\n",
        "- bmi - 14 - 0.4 (continuous)\n",
        "\n",
        "- heart_rate - 1 - 0.02(continuous)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZXfIDJlMJgR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Education and Bp-meds (mode)\n",
        "\n",
        "# Replacing the missing values in the categorical columns with its mode\n",
        "df['education'] = df['education'].fillna(df['education'].mode()[0])\n",
        "df['bp_meds'] = df['bp_meds'].fillna(df['bp_meds'].mode()[0])"
      ],
      "metadata": {
        "id": "N2TzbiV_6AcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cigs_per_day, Total cholesterol, Bmi, Glucose (mean)\n",
        "\n",
        "# Replacing missing values in the continuous columns with it mean\n",
        "\n",
        "df['cigs_per_day'] = df['cigs_per_day'].fillna(df['cigs_per_day'].mean())\n",
        "df['total_cholesterol'] = df['total_cholesterol'].fillna(df['total_cholesterol'].mean())\n",
        "df['bmi'] = df['bmi'].fillna(df['bmi'].mean())\n",
        "df['glucose'] = df['glucose'].fillna(df['glucose'].mean())"
      ],
      "metadata": {
        "id": "d4J4f6NfHH79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Heart Rate (drop)\n",
        "\n",
        "# Removing missing value in heart rate as only one value is misssing\n",
        "\n",
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "_R2TP2YDIiAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "y9tbrjb8Gke8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have introduced 3 types of imputation techniques such as\n",
        "\n",
        " - Dropping the nan value\n",
        "\n",
        " - Substituting categorical values with the mode of the feature\n",
        "\n",
        " - Substituting continuous values with the mean of the features.\n",
        "\n",
        "These techniques were chosen according the prevalence of missing values in the data.  \n"
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Removing outliers by standard methods and Plotting graphs:\n",
        "df1 = df.copy()\n",
        "for col in continuous_var:\n",
        "  # Using IQR method to define the range of inliners:\n",
        "  q1, q3, median = df1[col].quantile([0.25,0.75,0.5])\n",
        "  lower_limit = q1 - 1.5*(q3-q1)\n",
        "  upper_limit = q3 + 1.5*(q3-q1)\n",
        "\n",
        "  # Replacing Outliers with median value\n",
        "  df1[col] = np.where(df[col] > upper_limit, median,np.where(\n",
        "                         df[col] < lower_limit,median,df1[col]))\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The technique used here is Interquartile Range (IQR) method. This helps in identifying and removing outliers in the continuous columns of the dataset.\n",
        "This technique was chosen because it is a robust method for detecting outliers that is not affected by the presence of extreme values. The IQR is calculated as the difference between the 75th and 25th percentiles of the data, and any value that falls below the 25th percentile minus 1.5 times the IQR or above the 75th percentile plus 1.5 times the IQR is considered an outlier. By using this method, I was able to identify and remove outliers in a consistent and objective manner."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "jKhNDyWEP0jG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the categorical column by get - dummies\n",
        "\n",
        "df1 = pd.get_dummies(df1, columns=['education'])\n",
        "df1 = pd.get_dummies(df1, columns=['sex'])\n",
        "df1 = pd.get_dummies(df1, columns=['is_smoking'])\n"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "0Wmo45MwQfuK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Onehot encoding is used to encode the education, sex and cigs_per_day columns.\n",
        "\n"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing - NOT REQUIRED\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "\n",
        "#removing multicollinearity by using VIF technique\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "def calc_vif(X):\n",
        "\n",
        "    # Calculating VIF\n",
        "    vif = pd.DataFrame()\n",
        "    vif[\"variables\"] = X.columns\n",
        "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "    return(vif)\n",
        "\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_feature_df = pd.DataFrame(df1[continuous_var])\n",
        "\n",
        "continuous_feature_df"
      ],
      "metadata": {
        "id": "iA084AW-SxRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "calc_vif(df1[[i for i in continuous_feature_df]])"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a new column pulse_pressure and dropping systolic_bp and diastolic_bp\n",
        "\n",
        "df1['pulse_pressure'] = df1['systolic_bp']-df1['diastolic_bp']\n",
        "df1.drop('systolic_bp',axis=1,inplace=True)\n",
        "df1.drop('diastolic_bp',axis=1,inplace=True)\n"
      ],
      "metadata": {
        "id": "i8wRvih5vbXX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing systolic_bp and diastolic_bp. Appending pulse_pressure.\n",
        "continuous_var.remove('systolic_bp')\n",
        "continuous_var.remove('diastolic_bp')\n",
        "continuous_var.append('pulse_pressure')\n"
      ],
      "metadata": {
        "id": "Uq49Fsh_wajU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "continuous_feature_df = pd.DataFrame(df1[continuous_var])"
      ],
      "metadata": {
        "id": "k95HIueSwfU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "calc_vif(df1[[i for i in continuous_feature_df]])"
      ],
      "metadata": {
        "id": "gqEneTdAwh3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corr = df1[continuous_var].corr()\n",
        "mask = np.zeros_like(corr)\n",
        "\n",
        "mask[np.triu_indices_from(mask)] = True\n",
        "\n",
        "with sns.axes_style(\"white\"):\n",
        "    f, ax = plt.subplots(figsize=(18, 9))\n",
        "    ax = sns.heatmap(corr , mask=mask, vmin = -1,vmax=1, annot = True, cmap=\"YlGnBu\")"
      ],
      "metadata": {
        "id": "L3R1zpitwl9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used variance inflation factor to remove multicollinearity and we found that the systolic and diastolic blood pressure have high VIF, so we created a new feature which is pulse pressure.\n",
        "\n"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally the important colums are 'age', 'sex', 'cigs_per_day', 'bp_meds', 'prevalent_stroke', 'prevalent_hyp', 'diabetes', 'total_cholesterol', 'bmi', 'heart_rate', 'glucose', 'ten_year_chd', 'education_1.0', 'education_2.0', 'education_3.0', 'education_4.0', 'pulse_pressure'.\n",
        "\n",
        "All these columns contains the demographic, behavioural, current medical and historic medical data."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transform Your data\n",
        "# skewness along the index axis\n",
        "(df1[continuous_var]).skew(axis = 0)\n"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Skew for sqrt transformation\n",
        "np.sqrt(df1[continuous_var]).skew(axis = 0)"
      ],
      "metadata": {
        "id": "r5l6_JsrxQn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Skew for log10 transformation\n",
        "np.log10(df1[continuous_var]+1).skew(axis = 0)"
      ],
      "metadata": {
        "id": "ANrreecMxTso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing log transformation on continuous variables\n",
        "\n",
        "df1['age']                   = np.log10(df1['age']+1)\n",
        "df1['cigs_per_day']          = np.sqrt(df1['cigs_per_day'])\n",
        "df1['total_cholesterol']     = np.log10(df1['total_cholesterol']+1)\n",
        "df1['bmi']                   = np.sqrt(df1['bmi']+1)\n",
        "df1['heart_rate']            = np.log10(df1['heart_rate']+1)\n",
        "df1['glucose']               = np.sqrt(df1['glucose'])\n",
        "df1['pulse_pressure']        = np.sqrt(df1['pulse_pressure'])\n"
      ],
      "metadata": {
        "id": "YFmnBhlXxXHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Checking skew after log transformation\n",
        "df1[continuous_var].skew(axis = 0)"
      ],
      "metadata": {
        "id": "QbtSWabDxahT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes the data need to be transformed as it was skewed.\n",
        "\n",
        "We used log transform and squareroot transform on the different continuous columns to reduce the skew of the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZxE2wdLexctW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Scaling your data\n",
        "scaler = StandardScaler()\n",
        "features = [i for i in df1.columns if i not in ['ten_year_chd']]\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "features"
      ],
      "metadata": {
        "id": "7faL6O71xlYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "continuous_var"
      ],
      "metadata": {
        "id": "iUh-N3pExpG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df1[continuous_var] = scaler.fit_transform(df1[continuous_var])"
      ],
      "metadata": {
        "id": "423fc0O0xuUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# defining the X and y\n",
        "X = df1.drop('ten_year_chd',axis=1)\n",
        "y = df1['ten_year_chd']"
      ],
      "metadata": {
        "id": "iZfSxee2xvO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?\n",
        "\n"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this we have different independent features of different scale so we have used standard scalar method to scale our independent features into one scale.\n"
      ],
      "metadata": {
        "id": "qrNSSRlGF6vJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction is not needed. We have already reduced the number of features and only the important features are left."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3697, stratify=y, shuffle=True)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.value_counts()"
      ],
      "metadata": {
        "id": "FUCujKaDyq1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To train the model we have split the data into train and test using train_test_split method\n",
        "\n",
        "We have split 80% of our data into train and 20% into test, this ratio provides a good balance between having enough data to train a model effectively and having enough data to evaluate the model’s performance on unseen data. By using 80% of the data for training, the model has access to a large amount of information to learn from, while the remaining 20% of the data can be used to assess how well the model generalizes to new data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "# visualize the target variable before SMOTE\n",
        "y_train.value_counts().plot(kind='bar', title='Target variable before SMOTE')"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversampling using SMOTETomek\n",
        "# fit predictor and target variable\n",
        "X_smote, y_smote = SMOTETomek(random_state=0).fit_resample(X_train, y_train)\n",
        "\n",
        "print('Samples in the original dataset: ', len(y_train))\n",
        "print('Samples in the resampled dataset: ', len(y_smote))"
      ],
      "metadata": {
        "id": "fmXLsuOC4lnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualize the target variable after SMOTE\n",
        "y_smote.value_counts().plot(kind='bar', title='Target variable after SMOTE')"
      ],
      "metadata": {
        "id": "UxOI8wmT4o61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technique used here is called SMOTE combined with Tomek links oversampling. This technique helps to balance the distribution of classes in the dataset.\n",
        "\n",
        "SMOTE works by creating new samples for the minority class. It does this by finding instances that are similar to each other and drawing a line between them. Along this line, new instances are generated to increase the representation of the minority class.\n",
        "\n",
        "Tomek links oversampling, focuses on removing pairs of instances that belong to different classes but are very close to each other in the feature space. By removing these instances, the decision boundary between the classes becomes clearer.\n",
        "\n"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 | Logistic Regression"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''The function will take model, x train, x test, y train, y test\n",
        "    and then it will fit the model, then make predictions on the trained model,\n",
        "    it will then print roc-auc score of train and test, then plot the roc, auc curve,\n",
        "    print confusion matrix for train and test, then print classification report for train and test,\n",
        "    then plot the feature importances if the model has feature importances,\n",
        "    and finally it will return the following scores as a list:\n",
        "    recall_train, recall_test, acc_train, acc_test, roc_auc_train, roc_auc_test, F1_train, F1_test\n",
        "    '''\n",
        "\n",
        "def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "\n",
        "    # fit the model on the training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # make predictions on the test data\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test = model.predict(X_test)\n",
        "    pred_prob_train = model.predict_proba(X_train)[:,1]\n",
        "    pred_prob_test = model.predict_proba(X_test)[:,1]\n",
        "\n",
        "    # calculate ROC AUC score\n",
        "    roc_auc_train = roc_auc_score(y_train, y_pred_train)\n",
        "    roc_auc_test = roc_auc_score(y_test, y_pred_test)\n",
        "    print(\"\\nTrain ROC AUC:\", roc_auc_train)\n",
        "    print(\"Test ROC AUC:\", roc_auc_test)\n",
        "\n",
        "    # plot the ROC curve\n",
        "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, pred_prob_train)\n",
        "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, pred_prob_test)\n",
        "    plt.plot([0,1],[0,1],'k--')\n",
        "    plt.plot(fpr_train, tpr_train, label=\"Train ROC AUC: {:.2f}\".format(roc_auc_train))\n",
        "    plt.plot(fpr_test, tpr_test, label=\"Test ROC AUC: {:.2f}\".format(roc_auc_test))\n",
        "    plt.legend()\n",
        "    plt.title(\"ROC Curve\")\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.show()\n",
        "\n",
        "    # calculate confusion matrix\n",
        "    cm_train = confusion_matrix(y_train, y_pred_train)\n",
        "    cm_test = confusion_matrix(y_test, y_pred_test)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    sns.heatmap(cm_train, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"BuPu\", fmt='.4g', ax=ax[0])\n",
        "    ax[0].set_xlabel(\"Predicted Label\")\n",
        "    ax[0].set_ylabel(\"True Label\")\n",
        "    ax[0].set_title(\"Train Confusion Matrix\")\n",
        "\n",
        "    sns.heatmap(cm_test, annot=True, xticklabels=['Negative', 'Positive'], yticklabels=['Negative', 'Positive'], cmap=\"BuPu\", fmt='.4g', ax=ax[1])\n",
        "    ax[1].set_xlabel(\"Predicted Label\")\n",
        "    ax[1].set_ylabel(\"True Label\")\n",
        "    ax[1].set_title(\"Test Confusion Matrix\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "    # calculate classification report\n",
        "    cr_train = classification_report(y_train, y_pred_train, output_dict=True)\n",
        "    cr_test = classification_report(y_test, y_pred_test, output_dict=True)\n",
        "    print(\"\\nTrain Classification Report:\")\n",
        "    crt = pd.DataFrame(cr_train).T\n",
        "    print(crt.to_markdown())\n",
        "    # sns.heatmap(pd.DataFrame(cr_train).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
        "    print(\"\\nTest Classification Report:\")\n",
        "    crt2 = pd.DataFrame(cr_test).T\n",
        "    print(crt2.to_markdown())\n",
        "    # sns.heatmap(pd.DataFrame(cr_test).T.iloc[:, :-1], annot=True, cmap=\"Blues\")\n",
        "\n",
        "    try:\n",
        "      try:\n",
        "        feature_importance = model.feature_importances_\n",
        "      except:\n",
        "        feature_importance = model.coef_\n",
        "      feature_importance = np.absolute(feature_importance)\n",
        "      if len(feature_importance)==len(features):\n",
        "        pass\n",
        "      else:\n",
        "        feature_importance = feature_importance[0]\n",
        "\n",
        "\n",
        "      feat = pd.Series(feature_importance, index=features)\n",
        "      feat = feat.sort_values(ascending=True)\n",
        "      plt.figure(figsize=(10,6))\n",
        "      plt.title('Feature Importances for '+str(model), fontsize = 18)\n",
        "      plt.xlabel('Relative Importance')\n",
        "      feat.plot(kind='barh')\n",
        "    except AttributeError:\n",
        "        print(\"\\nThe model does not have feature importances attribute.\")\n",
        "\n",
        "    precision_train = cr_train['weighted avg']['precision']\n",
        "    precision_test = cr_test['weighted avg']['precision']\n",
        "\n",
        "    recall_train = cr_train['weighted avg']['recall']\n",
        "    recall_test = cr_test['weighted avg']['recall']\n",
        "\n",
        "    acc_train = accuracy_score(y_true = y_train, y_pred = y_pred_train)\n",
        "    acc_test = accuracy_score(y_true = y_test, y_pred = y_pred_test)\n",
        "\n",
        "    F1_train = cr_train['weighted avg']['f1-score']\n",
        "    F1_test = cr_test['weighted avg']['f1-score']\n",
        "\n",
        "    model_score = [precision_train, precision_test, recall_train, recall_test, acc_train, acc_test, roc_auc_train, roc_auc_test, F1_train, F1_test ]\n",
        "    return model_score\n",
        "\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a score dataframe\n",
        "score = pd.DataFrame(index = ['Precision Train', 'Precision Test','Recall Train','Recall Test','Accuracy Train', 'Accuracy Train', 'ROC AUC Train', 'ROC AUC Test', 'F1 Train', 'F1 Train'])\n"
      ],
      "metadata": {
        "id": "tcoBtoYHHDhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 1 Implementation\n",
        "lr_model = LogisticRegression(fit_intercept=True, max_iter=10000)\n",
        "\n",
        "# model is trained (fit ) and predicted in the evaluate model\n",
        "\n"
      ],
      "metadata": {
        "id": "mGUIj0q-Hm9z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score = evaluate_model(lr_model, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score['Logistic regression'] = lr_score\n",
        "score"
      ],
      "metadata": {
        "id": "Xa_Qtr_5IHqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "param_grid = {'C': [100,10,1,0.1,0.01,0.001,0.0001],\n",
        "              'penalty': ['l1', 'l2'],\n",
        "              'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "# Initializing the logistic regression model\n",
        "logreg = LogisticRegression(fit_intercept=True, max_iter=10000, random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=4, random_state=0)\n",
        "\n",
        "# Using GridSearchCV to tune the hyperparameters using cross-validation\n",
        "grid = GridSearchCV(logreg, param_grid, cv=rskf)\n",
        "grid.fit(X_smote, y_smote)\n",
        "\n",
        "best_params = grid.best_params_\n",
        "# The best hyperparameters found by GridSearchCV\n",
        "print(\"Best hyperparameters: \", best_params)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model with best parameters\n",
        "lr_model2 = LogisticRegression(C=best_params['C'],\n",
        "                                  penalty=best_params['penalty'],\n",
        "                                  solver=best_params['solver'],\n",
        "                                  max_iter=10000, random_state=0)\n"
      ],
      "metadata": {
        "id": "3_qIeIBZIXi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "lr_score2 = evaluate_model(lr_model2, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "c78EBO4IIaXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score['Logistic regression tuned'] = lr_score2"
      ],
      "metadata": {
        "id": "xllPPGhJIfZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The hyperparameter optimization technique used is GridSearchCV. GridSearchCV is a method that performs an exhaustive search over a specified parameter grid to find the best hyperparameters for a model. It is a popular method for hyperparameter tuning because it is simple to implement and can be effective in finding good hyperparameters for a model.\n",
        "\n",
        "The choice of hyperparameter optimization technique depends on various factors such as the size of the parameter space, the computational resources available, and the time constraints. GridSearchCV can be a good choice when the parameter space is relatively small and computational resources are not a major concern."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the score to evaluate improvement\n",
        "score\n"
      ],
      "metadata": {
        "id": "Uase70-zImp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning did not improve the performance of the Logistic Regression model on the test set. The precision, recall, accuracy, ROC-AUC, and F1 scores on the test set are the same for both the untuned and tuned Logistic Regression models."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 | SVM"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model | SVM\n",
        "svm = SVC(kernel='linear', random_state=0, probability=True)"
      ],
      "metadata": {
        "id": "iFhZVc1rJKgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "\n",
        "# Visualizing evaluation Metric Score chart\n",
        "svm_score = evaluate_model(svm, X_smote, X_test, y_smote, y_test)\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {'C': np.arange(0.1, 10, 0.1),\n",
        "              'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "              'degree': np.arange(2, 6, 1)}\n",
        "\n",
        "# Initialize the model\n",
        "svm2 = SVC(random_state=0, probability=True)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV with 6-fold cross-validation\n",
        "random_search = RandomizedSearchCV(svm2, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize model with best parameters\n",
        "svm2 = SVC(C = best_params['C'],\n",
        "           kernel = best_params['kernel'],\n",
        "           degree = best_params['degree'],\n",
        "           random_state=0, probability=True)"
      ],
      "metadata": {
        "id": "mSys6qG4KQqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "svm2_score = evaluate_model(svm2, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "secen-HjKTBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score['SVM tuned'] = svm2_score"
      ],
      "metadata": {
        "id": "TZXdsoJ_KWb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here Randomized search is used. Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "score"
      ],
      "metadata": {
        "id": "FBXiGrJQKn1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the SVM model on the test set. The tuned SVM model has higher recall, accuracy, and F1 score on the test set compared to the untuned SVM model. However, the precision and ROC-AUC scores on the test set decreased slightly after tuning"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initiate model\n",
        "neural = MLPClassifier(random_state=0)"
      ],
      "metadata": {
        "id": "hQjjqadBK6dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural_score = evaluate_model(neural, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score['Neural Network'] = neural_score\n",
        "score"
      ],
      "metadata": {
        "id": "g1WUR-UWLATR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural_score = evaluate_model(neural, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {'hidden_layer_sizes': np.arange(10, 100, 10),\n",
        "              'alpha': np.arange(0.0001, 0.01, 0.0001)}\n",
        "# Initialize the model\n",
        "neural = MLPClassifier(random_state=0)\n",
        "\n",
        "# repeated stratified kfold\n",
        "rskf = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=0)\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(neural, param_grid, n_iter=10, cv=rskf, n_jobs=-1)\n",
        "\n",
        "# Fit the RandomizedSearchCV to the training data\n",
        "random_search.fit(X_smote, y_smote)\n",
        "\n",
        "# Select the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "best_params\n"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initiate model with best parameters\n",
        "neural2 = MLPClassifier(hidden_layer_sizes = best_params['hidden_layer_sizes'],\n",
        "                        alpha = best_params['alpha'],\n",
        "                        random_state = 0)"
      ],
      "metadata": {
        "id": "sVA9H9ZENFyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "neural2_score = evaluate_model(neural2, X_smote, X_test, y_smote, y_test)"
      ],
      "metadata": {
        "id": "1-L1HBItNH7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score['Neural Network tuned']=neural2_score"
      ],
      "metadata": {
        "id": "D13Bprj2NOfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have used Randomized search to tune the Neural Network model.\n",
        "\n",
        "Randomized search is a popular technique because it can be more efficient than exhaustive search methods like grid search. Instead of trying all possible combinations of hyperparameters, randomized search samples a random subset of the hyperparameter space. This can save time and computational resources while still finding good hyperparameters for the model"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score"
      ],
      "metadata": {
        "id": "5vqtVgRONYap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that hyperparameter tuning improved the performance of the neural network model on the test set. The tuned neural network has higher precision, recall, accuracy, and F1 score on the test set compared to the untuned neural network. The ROC-AUC score on the test set also improved slightly after tuning."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(score.to_markdown())"
      ],
      "metadata": {
        "id": "uCfbvWZ8NlhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot of scores for models"
      ],
      "metadata": {
        "id": "rkqkCm54No9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Precision\n",
        "models = list(score.columns)\n",
        "train = score.iloc[0,:]\n",
        "test = score.iloc[1,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train Precision')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test Precision')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"Precision Score\")\n",
        "plt.title(\"Precision score for each model\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "VzTxHWgkNsZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "D-rn3BW8NzRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[2,:]\n",
        "test = score.iloc[3,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train Recall')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test Recall')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"Recall Score\")\n",
        "plt.title(\"Recall score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "arOMAul5Nmjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy\n",
        "models = list(score.columns)\n",
        "train = score.iloc[4,:]\n",
        "test = score.iloc[5,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train Accuracy')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test Accuracy')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"Accuracy Score\")\n",
        "plt.title(\"Accuracy score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GXM1YRd3N4e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC-AUC\n",
        "\n",
        "# ROC-AUC Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[6,:]\n",
        "test = score.iloc[7,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train ROC-AUC')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test ROC-AUC')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"ROC-AUC Score\")\n",
        "plt.title(\"ROC-AUC score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DG4SciHpOD5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# F1 macro Scores plot\n",
        "\n",
        "models = list(score.columns)\n",
        "train = score.iloc[8,:]\n",
        "test = score.iloc[9,:]\n",
        "\n",
        "X_axis = np.arange(len(models))\n",
        "\n",
        "plt.figure(figsize=(25,10))\n",
        "plt.bar(X_axis - 0.2, train, 0.4, label = 'Train F1 macro')\n",
        "plt.bar(X_axis + 0.2, test, 0.4, label = 'Test F1 macro')\n",
        "\n",
        "\n",
        "plt.xticks(X_axis,models, rotation=30)\n",
        "plt.ylabel(\"F1 macro Score\")\n",
        "plt.title(\"F1 macro score for each model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qhLZBmCuOGlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score"
      ],
      "metadata": {
        "id": "pq1sRQypOUpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the overfitted models which have recall, rocauc, f1 for train as 1\n",
        "score_t = score.transpose()            #taking transpose of the score dataframe to create new difference column\n",
        "remove_models = score_t[score_t['Recall Train']>=0.95].index  #creating a list of models which have 1 for train and score_t['Accuracy Train']==1.0 and score_t['ROC-AUC Train']==1.0 and score_t['F1 macro Train']==1.0\n",
        "remove_models\n",
        "\n",
        "adj = score_t.drop(remove_models)                     #creating a new dataframe with required models\n",
        "adj\n"
      ],
      "metadata": {
        "id": "n8EA6XloOZ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def select_best_model(df, metrics):\n",
        "\n",
        "    best_models = {}\n",
        "    for metric in metrics:\n",
        "        max_test = df[metric + ' Test'].max()\n",
        "        best_model_test = df[df[metric + ' Test'] == max_test].index[0]\n",
        "        best_model = best_model_test\n",
        "        best_models[metric] = best_model\n",
        "    return best_models"
      ],
      "metadata": {
        "id": "4jd1qqSfOdO2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "metrics = ['Precision','Recall', 'Accuracy', 'ROC-AUC', 'F1 macro']\n",
        "best_models = select_best_model(adj, metrics)\n",
        "print(\"The best models are:\")\n",
        "for metric, best_model in best_models.items():\n",
        "    print(f\"{metric}: {best_model} - {adj[metric+' Test'][best_model].round(4)}\")"
      ],
      "metadata": {
        "id": "JvjWi7LCOjch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After carefully considering the potential consequences of false positives and false negatives in the context of our business objectives, I have selected recall as the primary evaluation metric for our CHD risk prediction model. This means that our goal is to maximize the number of true positives (patients correctly identified as having CHD risk) while minimizing the number of false negatives (patients incorrectly identified as not having CHD risk). By doing so, we aim to ensure that we correctly identify as many patients with CHD risk as possible, even if it means that we may have some false positives."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After evaluating the performance of several machine learning models on the Framingham Heart Study dataset, I have selected the Neural Network (tuned) as our final prediction model. This decision was based on the model’s performance on our primary evaluation metric of recall, which measures the ability of the model to correctly identify patients with CHD risk. In our analysis, we found that the Neural Network (tuned) had the highest recall score among the models we evaluated.\n",
        "\n",
        "We chose recall as our primary evaluation metric because correctly identifying patients with CHD risk is critical to achieving our business objectives. By selecting a model with a high recall score, we aim to ensure that we correctly identify as many patients with CHD risk as possible, even if it means that we may have some false positives. Overall, we believe that the Neural Network (tuned) is the best choice for our needs and will help us achieve a positive business impact."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "id": "rj-TPrtDO2W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# importing shap\n",
        "import shap\n"
      ],
      "metadata": {
        "id": "hOHwC5tLO5ET"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X"
      ],
      "metadata": {
        "id": "kQRMWWzrO7Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# summarize the background dataset using k-means clustering\n",
        "X_summary = shap.kmeans(X, 100)\n",
        "\n",
        "# create an explainer object\n",
        "explainer = shap.KernelExplainer(neural2.predict_proba, X_summary)\n",
        "\n",
        "# compute the SHAP values for all the samples in the test data\n",
        "shap_values = explainer.shap_values(X_test)"
      ],
      "metadata": {
        "id": "TPyc01EzO-pF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Summery plot\n",
        "shap.summary_plot(shap_values, X_test, feature_names=features)"
      ],
      "metadata": {
        "id": "JPOX0TcDPBqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar plot shows the important features and the mean shap values. It shows the average impact on the model output magnitude.\n",
        "\n",
        "It does not show the positive or negative impact on the prediction."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In conclusion, this project demonstrated the potential of machine learning techniques to accurately predict the 10-year risk of future coronary heart disease (CHD) in patients using data from an ongoing cardiovascular study. Key points from this project include:\n",
        "\n",
        "- Careful data preprocessing and transformation improved the performance of machine learning models and enabled more accurate predictions.\n",
        "\n",
        "- Feature selection was important for identifying the most relevant predictors of CHD risk.\n",
        "\n",
        "- The Neural Network model (tuned) was chosen as the final prediction model due to its high recall score.\n",
        "\n",
        "- Techniques such as SMOTE combined with Tomek links undersampling and standard scalar scaling were used to handle imbalanced data and improve model performance.\n",
        "\n",
        "- This project provides a valuable example of how machine learning techniques can be applied to real-world problems to achieve positive business impact.\n",
        "\n",
        "Overall, this project highlights the importance of careful data preparation and analysis in machine learning projects. By taking the time to clean and transform the data, select relevant features, and choose an appropriate model, it is possible to achieve accurate predictions and support decision-making in a wide range of domains.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    }
  ]
}